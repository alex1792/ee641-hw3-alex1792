<!DOCTYPE html><html><head>
      <title>report</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/yuhungkung/.cursor/extensions/shd101wyy.markdown-preview-enhanced-0.8.19-universal/crossnote/dependencies/katex/katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="ee-641---homework-3-attention-mechanisms-and-transformers">EE 641 - Homework 3: Attention Mechanisms and Transformers </h1>
<ul>
<li><strong>Name</strong>: Yu Hung Kung</li>
<li><strong>USCID</strong>: 3431428440</li>
<li><strong>Email</strong>: <a href="mailto:yuhungku@usc.edu">yuhungku@usc.edu</a></li>
</ul>
<h2 id="problem-1">Problem 1 </h2>
<h3 id="overview">Overview </h3>
<p>This problem implements multi-head attention mechanisms from scratch and applies them to a sequence-to-sequence multi-digit addition task. The model must learn to add two 3-digit numbers with carry propagation, mapping input sequences like <code>[3, 7, 4, +, 8, 1, 4]</code> to output sequences like <code>[1, 1, 8, 8]</code> (374 + 814 = 1188).</p>
<h3 id="experimental-setup">Experimental Setup </h3>
<ul>
<li><strong>Task</strong>: Multi-digit addition with carry propagation</li>
<li><strong>Architecture</strong>: Encoder-decoder transformer with 2 encoder layers, 2 decoder layers, 4 attention heads per layer</li>
<li><strong>Model Dimensions</strong>: d_model=128, d_ff=512, num_heads=4</li>
<li><strong>Training Data</strong>: 10,000 samples (3-digit addition problems)</li>
<li><strong>Validation Data</strong>: 2,000 samples</li>
<li><strong>Test Data</strong>: 2,000 samples</li>
</ul>
<h3 id="1-training-results">1. Training Results </h3>
<p>The model was trained for 50 epochs with the following progression:</p>
<ul>
<li><strong>Initial Performance</strong>: Training accuracy started at ~1.1%, indicating the model begins from near-random performance</li>
<li><strong>Rapid Learning Phase</strong>: Around epoch 16-17, the model shows a dramatic improvement from ~65% to ~84% accuracy</li>
<li><strong>Final Performance</strong>:
<ul>
<li>Training accuracy: 98.2%</li>
<li>Validation accuracy: 99.8%</li>
<li>Test accuracy: 100% (sequence-level exact match)</li>
</ul>
</li>
</ul>
<p>The training loss decreased from 2.25 to below 0.01, demonstrating successful convergence. The model achieves near-perfect performance on the test set when using teacher forcing during training.</p>
<h3 id="2-attention-pattern-analysis">2. Attention Pattern Analysis </h3>
<p>Analysis of attention patterns reveals different heads specialize in distinct aspects of the addition task:</p>
<h4 id="encoder-self-attention-patterns">Encoder Self-Attention Patterns </h4>
<p><img src="problem1/results/attention_patterns/encoder_head_1_all_samples.png" alt="Encoder Attention Head 1"><br>
<img src="problem1/results/attention_patterns/encoder_head_2_all_samples.png" alt="Encoder Attention Head 2"><br>
<img src="problem1/results/attention_patterns/encoder_head_3_all_samples.png" alt="Encoder Attention Head 3"><br>
<img src="problem1/results/attention_patterns/encoder_head_4_all_samples.png" alt="Encoder Attention Head 4"></p>
<p><em>Figure 1: Encoder self-attention patterns across different heads and samples. Each subplot shows how input positions attend to each other.</em></p>
<p><strong>Key Observations:</strong></p>
<ol>
<li>
<p><strong>Position Alignment</strong>: Encoder attention primarily establishes correspondence between digits in the same position across the two numbers (hundreds↔hundreds, tens↔tens, ones↔ones). This is expected since the encoder's role is to understand the input structure.</p>
</li>
<li>
<p><strong>Operator Token Attention</strong>: Heads show varying levels of attention to the '+' operator token, with some heads (e.g., Head 1, Head 4) paying moderate attention (7.5% and 9.0% respectively) to this delimiter.</p>
</li>
<li>
<p><strong>No Direct Carry Information</strong>: Importantly, encoder attention patterns do <strong>not</strong> show direct connections to carry information, as carries are computed during the decoding process, not during encoding.</p>
</li>
</ol>
<h4 id="decoder-cross-attention-patterns">Decoder Cross-Attention Patterns </h4>
<p><img src="problem1/results/attention_patterns/decoder_cross_head_1_all_samples.png" alt="Decoder Cross-Attention Head 1"><br>
<img src="problem1/results/attention_patterns/decoder_cross_head_2_all_samples.png" alt="Decoder Cross-Attention Head 2"><br>
<img src="problem1/results/attention_patterns/decoder_cross_head_3_all_samples.png" alt="Decoder Cross-Attention Head 3"></p>
<p><em>Figure 2: Decoder cross-attention patterns showing how output positions attend to input positions. This is crucial for learning carry propagation.</em></p>
<p><strong>Key Observations:</strong></p>
<ol>
<li>
<p><strong>Digit-by-Digit Processing</strong>: Decoder cross-attention shows clear patterns where each output position attends to corresponding input digit positions, enabling the model to align digits for addition.</p>
</li>
<li>
<p><strong>Carry Propagation</strong>: Some heads exhibit attention patterns that suggest carry handling, though the visualization shows that the model struggles with thousand-digit carries (as evidenced by cases like 374 + 814 = 1188 being predicted as 0188).</p>
</li>
<li>
<p><strong>Head Specialization</strong>: Different heads show distinct attention distributions:</p>
<ul>
<li>Some heads focus more on operator positions</li>
<li>Others emphasize boundary positions (first/last digits)</li>
<li>Attention entropy varies from 2.19 (Head 2, very focused) to 5.88 (Head 0, more distributed)</li>
</ul>
</li>
</ol>
<h3 id="3-head-specialization-analysis">3. Head Specialization Analysis </h3>
<p>Quantitative analysis of head behavior reveals specialization:</p>
<table>
<thead>
<tr>
<th>Head</th>
<th>Diagonal Attention</th>
<th>Attention to Carry</th>
<th>Entropy</th>
<th>Sparsity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Head 0</td>
<td>0.385</td>
<td>0.083</td>
<td>5.88</td>
<td>0.312</td>
</tr>
<tr>
<td>Head 1</td>
<td>0.046</td>
<td>0.138</td>
<td>4.66</td>
<td>0.288</td>
</tr>
<tr>
<td><strong>Head 2</strong></td>
<td><strong>0.766</strong></td>
<td><strong>0.142</strong></td>
<td><strong>2.19</strong></td>
<td><strong>0.186</strong></td>
</tr>
<tr>
<td>Head 3</td>
<td>0.394</td>
<td>0.136</td>
<td>5.76</td>
<td>0.306</td>
</tr>
</tbody>
</table>
<p><strong>Findings:</strong></p>
<ol>
<li>
<p><strong>Head 2 Specialization</strong>: Head 2 shows the highest diagonal attention (0.766) and the lowest entropy (2.19), indicating it strongly focuses on position alignment. This head is critical for ensuring digits are added in the correct order.</p>
</li>
<li>
<p><strong>Carry Attention</strong>: Head 1 and Head 2 show the highest attention to carry positions (0.138 and 0.142), suggesting these heads are involved in carry propagation.</p>
</li>
<li>
<p><strong>High Entropy Heads</strong>: Heads 0 and 3 have high entropy (5.88 and 5.76), indicating more distributed attention patterns, potentially handling more complex relationships.</p>
</li>
</ol>
<h3 id="4-head-ablation-study">4. Head Ablation Study </h3>
<p><img src="problem1/results/head_analysis/head_importance.png" alt="Head Importance"></p>
<p><em>Figure 3: Performance drop when individual heads are removed. Higher bars indicate more critical heads.</em></p>
<p>The ablation study reveals significant differences in head importance:</p>
<h4 id="critical-heads-performance-drop--10">Critical Heads (Performance Drop &gt; 10%): </h4>
<ol>
<li><strong>decoder_layer_1_self_attn_head_2</strong>: 36.5% performance drop</li>
<li><strong>encoder_layer_1_self_attn_head_1</strong>: 40.05% performance drop</li>
<li><strong>decoder_layer_0_self_attn_head_1</strong>: 26.8% performance drop</li>
<li><strong>decoder_layer_1_cross_attn_head_1</strong>: 11.4% performance drop</li>
<li><strong>decoder_layer_1_self_attn_head_0</strong>: 19.5% performance drop</li>
</ol>
<h4 id="redundant-heads-performance-drop--0">Redundant Heads (Performance Drop = 0%): </h4>
<p>Many heads show zero performance drop when removed, indicating they are redundant:</p>
<ul>
<li>All encoder_layer_0 self-attention heads (heads 0, 1, 2, 3)</li>
<li>decoder_layer_0 cross-attention heads 1 and 2</li>
<li>decoder_layer_1 cross-attention heads 0, 2, and 3</li>
<li>decoder_layer_0 self-attention heads 0, 2</li>
<li>decoder_layer_1 self-attention head 1</li>
</ul>
<p><strong>Quantitative Analysis:</strong></p>
<ul>
<li><strong>Critical heads</strong>: 5 heads (20.8% of total 24 heads) show performance drop &gt; 10%</li>
<li><strong>Important heads</strong>: 8 heads show performance drop &gt; 1% but ≤ 10%</li>
<li><strong>Redundant heads</strong>: 11 heads (45.8%) show zero or negligible performance drop</li>
</ul>
<h3 id="5-discussion-how-attention-heads-specialize-for-carry-propagation">5. Discussion: How Attention Heads Specialize for Carry Propagation </h3>
<h4 id="understanding-carry-propagation">Understanding Carry Propagation </h4>
<p>Multi-digit addition with carry requires:</p>
<ol>
<li><strong>Digit Alignment</strong>: Matching digits in the same position (ones with ones, tens with tens, etc.)</li>
<li><strong>Local Addition</strong>: Adding aligned digits and computing sum and carry</li>
<li><strong>Carry Propagation</strong>: Passing carries from lower to higher positions</li>
<li><strong>Output Generation</strong>: Producing digits from right to left (ones to hundreds/thousands)</li>
</ol>
<h4 id="head-specialization-patterns">Head Specialization Patterns </h4>
<ol>
<li>
<p><strong>Encoder Heads</strong>: Focus on understanding input structure</p>
<ul>
<li>Establish digit correspondence between the two numbers</li>
<li>Identify operator positions</li>
<li><strong>Limited role in carry</strong>: Encoder cannot compute carries as it doesn't see output</li>
</ul>
</li>
<li>
<p><strong>Decoder Self-Attention Heads</strong>: Handle sequential dependencies</p>
<ul>
<li><strong>Critical head</strong>: decoder_layer_1_self_attn_head_2 shows 36.5% drop, indicating it's crucial for processing sequential output dependencies</li>
<li>Likely responsible for propagating information about previously generated digits (including carries)</li>
</ul>
</li>
<li>
<p><strong>Decoder Cross-Attention Heads</strong>: Connect encoder output to decoder positions</p>
<ul>
<li><strong>Key for carry computation</strong>: Cross-attention must identify which input positions contribute to each output digit</li>
<li>decoder_layer_1_cross_attn_head_1 shows 11.4% drop, suggesting it's involved in complex digit-to-digit mapping</li>
</ul>
</li>
</ol>
<h4 id="difficulties">Difficulties </h4>
<p>Despite high training/test accuracy, the model struggles with thousand-digit carries (e.g., 374 + 814 = 1188 predicted as 0188). This suggests:</p>
<ol>
<li>
<p><strong>Training Data Imbalance</strong>: Most 3-digit additions don't require thousand-digit carries, so the model learns to default to predicting 0 for the first position.</p>
</li>
<li>
<p><strong>Decoder First-Position Attention</strong>: When generating the first output token (thousand digit), the decoder cross-attention must aggregate information from all input positions (especially hundreds digits) to determine if a carry is needed. The attention patterns suggest this aggregation may be insufficient.</p>
</li>
<li>
<p><strong>Limited Generalization</strong>: The model may rely on patterns specific to the training distribution (where thousand-digit carries are rare) rather than learning the underlying arithmetic logic.</p>
</li>
</ol>
<h4 id="insights-on-head-redundancy">Insights on Head Redundancy </h4>
<p>The high number of redundant heads (55.2% show zero performance drop) suggests:</p>
<ol>
<li><strong>Over-parameterization</strong>: The model has more capacity than necessary for this task</li>
<li><strong>Redundancy</strong>: Multiple heads may learn similar patterns, making some interchangeable</li>
<li><strong>Pruning Opportunity</strong>: Nearly half the heads could be removed without performance loss, suggesting potential for model compression</li>
</ol>
<h3 id="6-quantitative-results-head-pruning-potential">6. Quantitative Results: Head Pruning Potential </h3>
<p>Based on the ablation study:</p>
<ul>
<li><strong>Heads that can be pruned with &lt;1% accuracy loss</strong>: 15 heads (62.5%)</li>
<li><strong>Heads that can be pruned with &lt;5% accuracy loss</strong>: 18 heads (75.0%)</li>
<li><strong>Critical heads that must be retained</strong>: 6 heads (25.0%)
<ul>
<li>decoder_layer_1_self_attn_head_2 (36.5% drop)</li>
<li>encoder_layer_1_self_attn_head_1 (40.05% drop)</li>
</ul>
</li>
</ul>
<h3 id="7-summary">7. Summary </h3>
<ol>
<li>
<p><strong>Attention Patterns</strong>: Different heads specialize in different aspects (position alignment, carry propagation, sequential dependencies)</p>
</li>
<li>
<p><strong>Head Importance</strong>: A small subset of heads (5 critical heads, 17.2%) are essential for performance, while most heads (55.2%) are redundant</p>
</li>
<li>
<p><strong>Carry Propagation Challenge</strong>: While the model achieves high accuracy on test set, it struggles with edge cases like thousand-digit carries, indicating limitations in generalization</p>
</li>
<li>
<p><strong>Model Efficiency</strong>: Significant pruning opportunities exist, suggesting the model could be compressed without substantial performance loss</p>
</li>
<li>
<p><strong>Architecture Insights</strong>: Decoder layers, especially layer 1, contain the most critical heads for this task, while encoder layer 0 appears largely redundant</p>
</li>
</ol>
<h2 id="problem-2">Problem 2 </h2>
<h3 id="overview-1">Overview </h3>
<p>This problem investigates the ability of different positional encoding strategies to generalize to sequence lengths beyond those seen during training. I train transformer models on sequences of length 8-16 and evaluate their performance on extrapolation lengths of 32, 64, 128, and 256 for a binary classification task: determining if a sequence of integers is sorted in ascending order.</p>
<h3 id="experimental-setup-1">Experimental Setup </h3>
<ul>
<li><strong>Task</strong>: Binary classification (sorted vs. unsorted sequences)</li>
<li><strong>Training</strong>: Sequences of length 8-16, integers 0-99</li>
<li><strong>Testing</strong>: Extrapolation to lengths 32, 64, 128, 256</li>
<li><strong>Positional Encoding Types</strong>: Sinusoidal, Learned, None</li>
<li><strong>Model Architecture</strong>: Transformer encoder with 4 layers, 4 attention heads, d_model=128</li>
</ul>
<h3 id="1-extrapolation-curves-analysis">1. Extrapolation Curves Analysis </h3>
<p><img src="figures/global_average_pooling/extrapolation_curves.png" alt="Extrapolation Curves"></p>
<p><em>Figure 4: Accuracy vs. sequence length for all three positional encoding methods. The training range (8-16) is highlighted in green.</em></p>
<p>The extrapolation results reveal interesting patterns across different positional encoding strategies:</p>
<h4 id="training-length-performance-8-16">Training Length Performance (8-16) </h4>
<ul>
<li><strong>Sinusoidal</strong>: Achieves 92.7% at length 8, increasing to 99.6% at length 16, demonstrating strong learning on training lengths.</li>
<li><strong>Learned</strong>: Shows poor performance (49.6% → 52.5% → 44.3%), indicating the model fails to learn effective patterns even within training range.</li>
<li><strong>None</strong>: Near-random performance (~48-54%), confirming that positional information is essential for this task.</li>
</ul>
<h4 id="extrapolation-length-performance-32-256">Extrapolation Length Performance (32-256) </h4>
<p>All three methods show dramatic performance degradation:</p>
<ul>
<li><strong>Sinusoidal</strong>: Drops from 54.6% at length 32 to ~48.5% at longer lengths (64, 128, 256)</li>
<li><strong>Learned</strong>: Identical performance to sinusoidal at extrapolation lengths (54.6% → 48.6% → 48.2% → 48.8%)</li>
<li><strong>None</strong>: Near-random across all extrapolation lengths (50% → 44.2% → 52.4% → 49.2%)</li>
</ul>
<h4 id="key-observations">Key Observations </h4>
<ol>
<li>While sinusoidal encoding achieves excellent performance on training lengths (99.6%), it fails to maintain this performance on extrapolation lengths, dropping to near-random levels.</li>
<li>The learned encoding performs poorly even on training lengths, suggesting it fails to learn effective positional representations.</li>
<li>Critically, <strong>all methods converge to similar performance (~48-50%) at extrapolation lengths</strong>, indicating the bottleneck is not the positional encoding type but rather the model architecture (specifically, the pooling strategy).</li>
</ol>
<h3 id="2-mathematical-explanation-why-sinusoidal-extrapolates-while-learned-encoding-fails">2. Mathematical Explanation: Why Sinusoidal Extrapolates While Learned Encoding Fails </h3>
<h4 id="sinusoidal-positional-encoding">Sinusoidal Positional Encoding </h4>
<p>The sinusoidal encoding uses a deterministic mathematical function:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>sin</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.854em;vertical-align:-0.704em;"></span><span class="mop">sin</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.296em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.704em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></span><br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.854em;vertical-align:-0.704em;"></span><span class="mop">cos</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.296em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.704em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></span></p>
<p><strong>Why Sinusoidal Encoding Can Extrapolate:</strong></p>
<ol>
<li>
<p><strong>Continuous Function</strong>: For any position <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> (even if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>&gt;</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">pos &gt; max\_len</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span>), the encoding can be computed directly using the trigonometric functions. There is no dependency on training data or learned parameters.</p>
</li>
<li>
<p><strong>Relative Position Preservation</strong>: The encoding preserves relative position relationships. For positions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">pos+k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>, their encoding relationship follows trigonometric identities:<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos+k) = f(PE(pos), k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span><br>
where the relationship is mathematically determined by the frequency components.</p>
</li>
<li>
<p><strong>Structural Regularity</strong>: The multi-scale frequency structure (different <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> values) provides hierarchical position information that scales naturally to any sequence length.</p>
</li>
<li>
<p><strong>Extrapolation Property</strong>: Mathematically, since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6679em;"></span><span class="mop">sin</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mop">cos</span></span></span></span> are defined for all real numbers, the encoding can be computed for any position:<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mn>0</mn></msup></mfrac><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mn>0</mn></msup></mfrac><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mn>2</mn></msup></mfrac><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">PE(pos) = [\sin(\frac{pos}{10000^0}), \cos(\frac{pos}{10000^0}), \sin(\frac{pos}{10000^2}), \ldots]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.7936em;vertical-align:-0.686em;"></span><span class="mopen">[</span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mclose">]</span></span></span></span></span></p>
</li>
</ol>
<h4 id="learned-positional-encoding">Learned Positional Encoding </h4>
<p>Learned encoding uses a lookup table:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Embedding</mtext><mo stretchy="false">[</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">PE(pos) = \text{Embedding}[pos]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Embedding</span></span><span class="mopen">[</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">]</span></span></span></span></span></p>
<p><strong>Why Learned Encoding Cannot Extrapolate:</strong></p>
<ol>
<li>
<p><strong>Discrete Embedding Table</strong>: Only positions from 0 to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">max\_len-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> have learned embedding vectors. Positions beyond this range have no corresponding embeddings.</p>
</li>
<li>
<p><strong>Clamping Problem</strong>: When <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>≥</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">pos \geq max\_len</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8304em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span>, the implementation uses:<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mtext>clamp</mtext><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mi>max</mi><mo>⁡</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos) = PE(\text{clamp}(pos, \max=max\_len-1)) = PE(max\_len-1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span><br>
This means all positions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>≥</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">pos \geq max\_len</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8304em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span> receive the same embedding as position <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">max\_len-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>, losing all positional discrimination.</p>
</li>
<li>
<p><strong>No Mathematical Structure</strong>: Unlike sinusoidal encoding, learned embeddings have no inherent mathematical relationship between positions. There is no formula to compute <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos+k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>.</p>
</li>
<li>
<p><strong>Training Dependency</strong>: The embeddings are learned only for positions seen during training. Without an extrapolation mechanism, unseen positions cannot be properly encoded.</p>
</li>
</ol>
<p><strong>Example</strong>: If <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">max\_len=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span> during training, positions 32, 64, 128, and 256 all map to position 15's embedding, making them indistinguishable.</p>
<h4 id="why-sinusoidal-still-fails-in-practice">Why Sinusoidal Still Fails in Practice </h4>
<p>Despite sinusoidal encoding's mathematical ability to extrapolate, my results show it fails on extrapolation lengths. This indicates:</p>
<ol>
<li>
<p><strong>Position Encoding is Not the Bottleneck</strong>: The model can receive correct positional information for any length, yet still fails.</p>
</li>
<li>
<p><strong>Architecture Limitation</strong>: The Global Average Pooling strategy dilutes important signals in long sequences. Key features that distinguish sorted from unsorted sequences get averaged out when the sequence length increases.</p>
</li>
<li>
<p><strong>Learning Strategy</strong>: The model may have learned patterns specific to short sequences (8-16) that don't generalize. The attention mechanism and classification head may rely on properties unique to short sequences.</p>
</li>
</ol>
<h3 id="3-position-embedding-visualization-analysis">3. Position Embedding Visualization Analysis </h3>
<p>The learned positional embeddings visualization (see <code>results/extrapolation/position_viz/learned_position_embeddings.png</code>) reveals:</p>
<h4 id="expected-characteristics">Expected Characteristics </h4>
<p><img src="figures/global_average_pooling/learned_position_embeddings.png" alt="Learned Position Embeddings"></p>
<p><em>Figure 5: Visualization of learned positional embeddings. Note the embeddings only exist for positions 0-15 (training range).</em></p>
<ol>
<li>
<p><strong>Training Range (0-15)</strong>: The embeddings should show some learned structure, potentially with smooth transitions between adjacent positions if the model learned relative position relationships.</p>
</li>
<li>
<p><strong>Limited Extrapolation</strong>: Since embeddings only exist for positions 0-15 (with max_len=16), there's no information about positions beyond this range, confirming the clamping behavior discussed above.</p>
</li>
</ol>
<h4 id="analysis-points">Analysis Points </h4>
<ul>
<li>
<p><strong>Discontinuity</strong>: If embeddings vary dramatically without smooth transitions, it suggests each position was learned independently without capturing relative position relationships.</p>
</li>
<li>
<p><strong>Patterns</strong>: Any smooth patterns in the visualization would indicate some relative position learning, but the fundamental limitation remains: no embeddings exist for positions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≥</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">\geq max\_len</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span>.</p>
</li>
<li>
<p><strong>Visual Confirmation</strong>: The heatmap should show embeddings only for the first 16 positions, with all extrapolation positions using the same clamped values, illustrating why learned encoding cannot distinguish between different long positions.</p>
</li>
</ul>
<h3 id="4-quantitative-comparison-accuracy-at-extrapolation-lengths">4. Quantitative Comparison: Accuracy at Extrapolation Lengths </h3>
<table>
<thead>
<tr>
<th>Sequence Length</th>
<th>Sinusoidal</th>
<th>Learned</th>
<th>None</th>
<th>Observations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>32</strong></td>
<td>54.6%</td>
<td>54.6%</td>
<td>50.0%</td>
<td>Sinusoidal and Learned perform identically, slightly above random</td>
</tr>
<tr>
<td><strong>64</strong></td>
<td>48.6%</td>
<td>48.6%</td>
<td>44.2%</td>
<td>All methods converge toward random performance</td>
</tr>
<tr>
<td><strong>128</strong></td>
<td>48.2%</td>
<td>48.2%</td>
<td>52.4%</td>
<td>None encoding performs best (by chance), all near random</td>
</tr>
<tr>
<td><strong>256</strong></td>
<td>48.8%</td>
<td>48.8%</td>
<td>49.2%</td>
<td>Minimal differences across all methods</td>
</tr>
</tbody>
</table>
<h4 id="key-findings">Key Findings </h4>
<ol>
<li>
<p><strong>Near-Random Performance</strong>: All methods achieve approximately 48-55% accuracy at extrapolation lengths, indicating near-random classification performance (expected random accuracy: 50%).</p>
</li>
<li>
<p><strong>Minimal Differences</strong>: The maximum difference between methods at any extrapolation length is ~6%, suggesting positional encoding type has minimal impact on extrapolation performance.</p>
</li>
<li>
<p><strong>Identical Performance</strong>: Sinusoidal and Learned encodings produce identical results at all extrapolation lengths (54.6%, 48.6%, 48.2%, 48.8%), further supporting that the bottleneck is architectural rather than positional encoding type.</p>
</li>
<li>
<p><strong>Training vs. Extrapolation Gap</strong>: The dramatic drop from 99.6% (training length 16) to ~49% (extrapolation lengths) for sinusoidal encoding highlights the severity of the extrapolation challenge.</p>
</li>
</ol>
<h3 id="5-discussion-and-insights">5. Discussion and Insights </h3>
<h4 id="why-all-methods-fail-at-extrapolation-lengths">Why All Methods Fail at Extrapolation Lengths </h4>
<p>The convergence of all three methods to near-random performance suggests:</p>
<ol>
<li>
<p><strong>Architecture Bottleneck</strong>: The Global Average Pooling strategy is the primary limitation. In short sequences, averaging preserves sufficient signal, but in long sequences, important features get diluted.</p>
</li>
<li>
<p><strong>Learning Pattern Mismatch</strong>: Models trained on short sequences may learn patterns that depend on sequence length-specific properties. These patterns don't generalize to longer sequences.</p>
</li>
<li>
<p><strong>Information Density</strong>: In longer sequences, the ratio of signal to noise decreases. The model needs more sophisticated aggregation strategies to extract relevant information.</p>
</li>
</ol>
<h4 id="implications-for-positional-encoding-research">Implications for Positional Encoding Research </h4>
<p>My results show that:</p>
<ul>
<li>
<p><strong>Positional encoding extrapolation capability</strong> (sinusoidal vs. learned) matters less than expected when the overall architecture cannot handle length generalization.</p>
</li>
<li>
<p><strong>Improving positional encoding</strong> alone is insufficient if other architectural components (pooling, attention mechanisms, classification heads) are length-sensitive.</p>
</li>
</ul>
<h3 id="6-improved-pooling-strategy">6. Improved Pooling Strategy </h3>
<h4 id="motivation">Motivation </h4>
<p>The previous analysis revealed that while positional encoding can mathematically extrapolate (in the case of sinusoidal encoding), all methods failed at extrapolation lengths, converging to near-random performance (~48-50% accuracy). This suggests that the bottleneck lies not in positional encoding but in the model's aggregation strategy.</p>
<p>The <strong>Global Average Pooling</strong> approach, while effective for short sequences, suffers from signal dilution in longer sequences. When a sequence contains only a few positions that violate the sorting order, their signals get averaged out by the majority of sorted positions. For example, in a 32-element sequence with one violation, the violation signal contributes only 1/32 of the final representation, making it difficult for the model to detect.</p>
<h4 id="proposed-strategy">Proposed Strategy </h4>
<p>To address this limitation, I implemented a multi-feature pooling strategy that combines several complementary signals:</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token comment"># Architecture modification</span>
self<span class="token punctuation">.</span>pool_projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>

<span class="token comment"># Improved pooling strategy</span>
<span class="token keyword keyword-if">if</span> mask <span class="token keyword keyword-is">is</span> <span class="token keyword keyword-not">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    lengths <span class="token operator">=</span> mask<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword keyword-for">for</span> i<span class="token punctuation">,</span> length <span class="token keyword keyword-in">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>
        seq <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span>length<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 1. Min pooling - captures violation signals</span>
        min_feat <span class="token operator">=</span> seq<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 2. Adjacent element differences</span>
        <span class="token keyword keyword-if">if</span> length <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            diffs <span class="token operator">=</span> seq<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> seq<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
            diff_min <span class="token operator">=</span> diffs<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># Most severe violation</span>
            diff_mean <span class="token operator">=</span> diffs<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>   <span class="token comment"># Average trend</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            diff_min <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> device<span class="token operator">=</span>seq<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            diff_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> device<span class="token operator">=</span>seq<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        
        <span class="token comment"># 3. Boundary information</span>
        first_feat <span class="token operator">=</span> seq<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        last_feat <span class="token operator">=</span> seq<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        
        <span class="token comment"># Combine features</span>
        combined <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>min_feat<span class="token punctuation">,</span> diff_min<span class="token punctuation">,</span> diff_mean<span class="token punctuation">,</span> first_feat<span class="token punctuation">,</span> last_feat<span class="token punctuation">]</span><span class="token punctuation">)</span>
        features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
    
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>features<span class="token punctuation">)</span>  <span class="token comment"># [batch, d_model * 5]</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool_projection<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre><p><strong>Key Components:</strong></p>
<ol>
<li>
<p><strong>Min Pooling</strong>: Extracts the minimum value across all positions for each dimension. In sorting detection, positions that violate the order typically have lower signal values, making min pooling effective at capturing these violations regardless of sequence length.</p>
</li>
<li>
<p><strong>Adjacent Element Differences</strong>: Computes the difference between consecutive elements:</p>
<ul>
<li><code>diff_min</code>: Captures the most negative difference, indicating the most severe violation of sorting order</li>
<li><code>diff_mean</code>: Represents the average trend, providing global ordering information</li>
<li>This directly addresses the core requirement of sorting detection: verifying that adjacent elements maintain the correct order</li>
</ul>
</li>
<li>
<p><strong>Boundary Information</strong>: Includes the first and last positions, which carry crucial information about sequence boundaries and can help identify edge cases in sorting violations.</p>
</li>
</ol>
<p>The combined features (5 × d_model dimensions) are then projected back to d_model dimensions using a learned linear projection, allowing the model to learn optimal feature combination.</p>
<h4 id="results">Results </h4>
<p>Using the same training setup (sequences of length 8-16) and testing on extrapolation lengths (32, 64, 128, 256), the improved pooling strategy demonstrates significant performance gains across all positional encoding methods:</p>
<p><img src="figures/my_pooling_strategy/extrapolation_curves.png" alt="Extrapolation Curves with Improved Pooling"></p>
<p><em>Figure 6: Accuracy vs. sequence length using the improved pooling strategy. The training range (8-16) is highlighted in green.</em></p>
<p><strong>Key Improvements:</strong></p>
<ul>
<li><strong>Sinusoidal encoding</strong>: Maintains high accuracy even at extrapolation lengths, showing that proper aggregation can leverage the extrapolation capability of positional encodings</li>
<li><strong>Learned encoding</strong>: Shows marked improvement, though still limited by its inability to extrapolate beyond training positions</li>
<li><strong>None encoding</strong>: Demonstrates that even without explicit positional encoding, better aggregation strategies can improve performance</li>
</ul>
<h4 id="analysis">Analysis </h4>
<p>The success of this strategy confirms our hypothesis that the bottleneck was in the aggregation mechanism rather than positional encoding. The multi-feature approach addresses several critical aspects:</p>
<ol>
<li>
<p><strong>Length-Invariant Signal Extraction</strong>: Min pooling and difference-based features maintain their discriminative power regardless of sequence length, as they focus on local violations rather than global averages.</p>
</li>
<li>
<p><strong>Direct Task Relevance</strong>: The adjacent element difference features directly encode the property I'm trying to detect (whether elements are in sorted order), making the representation more task-specific.</p>
</li>
<li>
<p><strong>Robustness</strong>: By combining multiple complementary signals, the model becomes more robust to noise and variations in sequence patterns.</p>
</li>
</ol>
<p>This improvement demonstrates that architectural choices in aggregation layers can be as important as positional encoding strategies for length generalization tasks.</p>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>